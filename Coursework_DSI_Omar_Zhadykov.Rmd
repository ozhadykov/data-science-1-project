---
title: "Coursework - Data Science I"
author: "Omar Zhadykov, 220220503"
output:
  html_notebook:
    fig_width: 10
    theme: spacelab
    toc: yes
    toc_depth: 3
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document: default
  html_document:
    fig_width: 10
    theme: spacelab
    toc: yes
    toc_depth: 3
    toc_float: yes
---

<script>
$(document).ready(function() {
  $items = $('div#TOC li');
  $items.each(function(idx) {
    num_ul = $(this).parentsUntil('#TOC').length;
    $(this).css({'text-indent': num_ul * 10, 'padding-left': 0});
  });

});
</script>

```{r setup, warning=FALSE, message=FALSE, echo=FALSE}
library(svglite)
library(knitr)
suppressPackageStartupMessages(library(data.table))
library(ggplot2)
knitr::opts_chunk$set(dev = "svglite")

# Put your dataset in the same folder as your R file. This code will set your working directory for this notebook to the folder where the R file is stored. This way I can rerun your code without modifications.

library(rstudioapi)
setwd(dirname(getActiveDocumentContext()$path))
```

# Introduction

This coursework focuses on housing prices, with the main objective being to predict the price of a property based on various inputs. The inputs include features such as the area, the number and types of rooms, and additional factors like the availability of a main road, hot water heating, and more.

The dependent variable is the price, as it is the primary concern for most people searching for a house. The goal of this work is to predict the price based on diverse inputs, which consist of mixed data types, such as:

  - Numerical values
  - Text-based responses like "yes" or "no"
  - Categories for furnishing status, including "furnished," "semi-furnished," or "non-furnished."

This project addresses a regression problem because the objective is to predict a numeric value—in this case, the price of the property.

# Collection / Preparation 

Now we are going to import our dataset into this project.

```{r}
dt_houses <- fread(file = "./datasets/Regression_set.csv")
```

<br>
I would like to check, if i have some nullish data in my dataset. I think it is a good idea to go through all rows and colums and check, if there is a NA. I want to check it with built-in function in R *complete.cases(data_table)*. This function returns TRUE or FALSE if row contains a NA value.

```{r}
nas <- dt_houses[!complete.cases(dt_houses)]
nas
```

That looks great, now we can explore our dataset :)

# Exploration

Before we will explore our data, I want to import all libraries, which we will probably use:

```{r}
library(data.table)
library(ggcorrplot)
library(ggExtra)
library(ggplot2)
library(ggridges)
library(ggsci)
library(ggthemes)
library(RColorBrewer)
library(svglite)
library(viridis)
library(scales)
library(rpart)
library(rpart.plot)
```

I found some helpful functions in R, so we could have a look on our data. We will start with a structure, than we will get some statistic data and take a *head()* of the data

```{r}
str(dt_houses)
```
<br>
Statistic data:
```{r}
summary(dt_houses[, .(price, area, bedrooms, bathrooms, stories, parking)])
```

<br>
and this is a sample of our dataset:

```{r}
head(dt_houses)
```

I would like to start from density of a main values, which are from my domain knowledge are important in price of the properties

Price density: 

```{r}
ggplot(data = dt_houses, aes(x = price)) + 
  geom_density(fill="#f1b147", color="#f1b147", alpha=0.25) + 
  labs(
    x = 'Price',
    y = 'Density'
  ) +
  geom_vline(xintercept = mean(dt_houses$price), linetype="dashed") + 
  scale_x_continuous(labels = label_number(scale = 1e-6, suffix = "M")) + 
  theme_minimal() + 
  theme(axis.line = element_line(color = "#000000"))
```
This density plot visualizes the distribution of house prices, showing that most houses are priced around 4-5 million, with a right-skewed distribution (some higher-priced houses pulling the mean up). The dashed vertical line represents the mean price (~5M). The plot highlights that while most houses fall within a moderate price range, some expensive properties extend beyond 10M.

Area density:

```{r}
ggplot(data = dt_houses, aes(x = area)) + 
  geom_density(fill="#f1b147", color="#f1b147", alpha=0.25) + 
  labs(
    x = 'Price',
    y = 'Density'
  ) +
  theme_minimal() + 
  theme(axis.line = element_line(color = "#000000"))
```
The area density plot looks similar to price density plot and can also make sense, because if house has a bigger area, the higher cost is quite expected. This plot shows that most houses are having area in range ~3000-5000. But some properties have area more than 12000.

<br>

Next plot will visualize the distribution of price depending on area. 

```{r}
ggplot() + 
  geom_point(data = dt_houses, aes(x = area, y = price, color = parking)) +
  scale_y_continuous(labels = label_number(scale = 1e-6, suffix = "M")) + 
  theme_minimal() + 
  theme(axis.line = element_line(color = "#000000"))
```

This scatter plot visualizes the relationship between house area (x-axis) and price (y-axis), with color indicating the number of parking spaces. It shows a positive correlation between area and price—larger houses tend to be more expensive. However, there is some variability, as some large houses have relatively lower prices. The color gradient suggests that houses with more parking spaces (lighter blue) tend to be higher in price and larger in area.

The next plot, which I am going to do is a boxplot and I want to use bedrooms as a factor variable on x axis and price on y-axis, to get an overall understanding, how amount of bedrooms affect price.

```{r}
ggplot(data = dt_houses, aes(x = factor(bedrooms), y = price)) +
  geom_boxplot() + 
  theme_minimal() 
```

Boxplot shows, that on average, houses with more bedrooms have higher prices, but around 4-6 bedrooms, 1 quantile stagnates, and so does median price. There are some outliers, but not too much.

It is also interesting to take a look at distribution of bedrooms, so next plot would be a histogram, because I want to know, which amount of bedrooms is the most "popular" in the whole dataset.

```{r}
ggplot(data = dt_houses, aes(x = bedrooms)) + 
  geom_histogram(fill="#2f9e44", color="#2f9e44", alpha=0.25) + 
  geom_vline(xintercept = mean(dt_houses$bedrooms), linetype="dashed") + 
  theme_minimal() + 
  theme(axis.line = element_line(color = "#000000"))
```
mean of the bedrooms:
```{r}
mean(dt_houses$bedrooms)
```

From this visualization we can mention, that the most of the houses have 2, 3 or 4 rooms. 1, 5 and 6 rooms are not as popular in this dataset.

Let's have a look at histogram of stories: 

```{r}
ggplot(data = dt_houses, aes(x = stories)) + 
  geom_histogram(fill="#2f9e44", color="#2f9e44", alpha=0.25) + 
  geom_vline(xintercept = mean(dt_houses$stories), linetype="dashed") + 
  theme_minimal() + 
  theme(axis.line = element_line(color = "#000000"))
```

```{r}
mean(dt_houses$stories)
```

This plot shows that most popular amount of stories are 1 and 2. 3 and 4 makeing less than 100 houses together.

Bathrooms are also interesting variable, so let's take a look at histogram and a Boxplot bathrooms and price:
```{r}
ggplot(data = dt_houses, aes(x = bathrooms)) + 
  geom_histogram(fill="#2f9e44", color="#2f9e44", alpha=0.25) + 
  geom_vline(xintercept = mean(dt_houses$bathrooms), linetype="dashed") + 
  theme_minimal() + 
  theme(axis.line = element_line(color = "#000000"))
```


```{r}
ggplot(data = dt_houses, aes(x = factor(bathrooms), y = price)) +
  geom_boxplot() + 
  theme_minimal() 
```

here it is also almost obvious, that, if we have more bathrooms, price will be also up. Only one disadvantage, that in my dataset I do not have enough data about properties with 3 or 4 bathrooms, I have some on 3, but really luck on 4.

Furnishing is also important, many people search for apartments with furniture, but furniture could be not in a best shape or buyer may do not like the style. So from my opinion, it is not as strong(in prediction), as for example area.

How much real estate furnished or not:

```{r}
ggplot(data = dt_houses, aes(x = factor(furnishingstatus), fill = factor(furnishingstatus))) + 
  geom_bar(color="#ced4da", alpha=0.25) + 
  scale_fill_viridis_d(option = "D") + 
  labs(title = "Bar Chart with Different Colors", 
       x = "Furnishing Status", 
       y = "Count") + 
  theme_minimal() + 
  theme(axis.line = element_line(color = "#000000"))
```

We can see, that most of the houses are semi-furnished. which is also logical, because when we sell a house or apartment, probably we would take in most of the cases the most valuable things for us and furniture included.

Now, it would be great, to look at price and area distribution in differently furnished properties


```{r}
ggplot(data = dt_houses, aes(y = price, x = area)) + 
  geom_point(data = dt_houses, aes(y = price, x = area, color = bedrooms)) +
  geom_hline(yintercept = mean(dt_houses$price), linetype='dashed') + 
  facet_grid(.~furnishingstatus) +
  scale_y_continuous(labels = label_number(scale = 1e-6, suffix = "M")) +
  scale_color_distiller(type = "seq", palette = "Greens") +
  theme_minimal() + 
  theme(axis.line = element_line(color = "#000000"))
```

Also, on average, you can notice, that unfurnished houses, are less expensive.

We can also take a look on some pie charts:

```{r}

dt_mainroad_counts <- as.data.frame(table(dt_houses$mainroad)) #table() - creates frequency table
colnames(dt_mainroad_counts) <- c("mainroad_status", "count")
dt_mainroad_counts$percentage <- round(dt_mainroad_counts$count / sum(dt_mainroad_counts$count) * 100, 1)

ggplot(data = dt_mainroad_counts, aes(x = "", y = count, fill = mainroad_status)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(percentage, "%")), 
            position = position_stack(vjust = 0.5), color = "white", size = 4) +  
  theme_void() +  
  scale_fill_manual(values = c("#F1B147", "#47B1F1")) + 
  labs(
    title = "Distribution of Mainroad Status",
    fill = "Mainroad Status"
  )

```

Almost 86 percent of houses have main road, so maybe this won't be a strong predictor variable.


```{r}

dt_airconditioning_counts <- as.data.frame(table(dt_houses$airconditioning)) #table() - creates frequency table
colnames(dt_airconditioning_counts) <- c("airconditioning_status", "count")
dt_airconditioning_counts$percentage <- round(dt_airconditioning_counts$count / sum(dt_airconditioning_counts$count) * 100, 1)

ggplot(data = dt_airconditioning_counts, aes(x = "", y = count, fill = airconditioning_status)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  geom_text(aes(label = paste0(percentage, "%")), 
            position = position_stack(vjust = 0.5), color = "white", size = 4) +  
  theme_void() +  
  scale_fill_manual(values = c("#F1B147", "#47B1F1")) + 
  labs(
    title = "Distribution of Airconditioning status",
    fill = "Airconditioning Status"
  )

```

Here 68.4 percent has airconditioning, but I do not know, how it will affect predictions.


I think that would be enough exploration and we can start with our first model.

# Models 1 & 2

First, I would like to start pretty simple with linear model.

I consider to take all variables to my model, because they all seem to be very important.

But before we start, I want to introduce a data table, which will be very useful in the end of this course work.

```{r}
dt_features_performance <- data.table("price_lm_rmse" = c(0, 0, 0, 0, 0), "price_tree_rmse" = c(0, 0, 0, 0, 0), "feature" = c(0, 1, 2, 3, 4))
```


## Linear model

I will use lm function in R to find needed beta coefficients and create my model

```{r}
price_lm <- lm(formula = price ~ area + bedrooms + hotwaterheating + airconditioning + stories + mainroad + parking + furnishingstatus + bathrooms + guestroom + basement + prefarea, data = dt_houses)

summary(price_lm)
```

We got 0.68 R-squared, which is not that bad for a model just made up. But that's not all, I will try to do better here, but first, another model.

But I would like to measure performance of my models with RMSE, so I will calculate RMSE for linear model.

```{r}
price_lm_rmse <- mean(sqrt(abs(price_lm$residuals)))

price_lm_rmse
```


## Tree Model

I think this model could perform better, because there some variables which can affect this model not only linearly, but the other way, in this case tree model can show better performance.

In this coursework will be used rpart to create a regression tree.

```{r}
prices_tree <- rpart(data = dt_houses, formula = price ~ area + bedrooms + hotwaterheating + airconditioning + stories + mainroad + parking + furnishingstatus + bathrooms + guestroom + basement + prefarea, method = 'anova')

prp(prices_tree, digits = -3)
```

```{r}
printcp(prices_tree)
```

Now after I have built with the help of rpart tree model based on my dataset, let us explore it:

```{r}
prices_tree
```

We can see, that we have 31 Nodes, I think for this kind of dataset it may be okay.

Now it would be great to prune the tree, because I do not want my tree to overfit:

```{r}
plotcp(prices_tree)
```
This is complexity of this tree. We need the lowest complexity, to get as few leafs as possible to get the best performance, so that tree won't overfit the data.

```{r}
prices_tree_min_cp <- prices_tree$cptable[which.min(prices_tree$cptable[, "xerror"]), "CP"]
model_tree <- prune(prices_tree, cp = prices_tree_min_cp )
prp(prices_tree,digits = -3)
```

after we pruned the tree, let's calculate the RMSE for the tree model


```{r}
prices_tree_pred <- predict(prices_tree, dt_houses[, c("area","bathrooms", "bedrooms", "hotwaterheating", "airconditioning", "parking", "stories", "mainroad", "furnishingstatus", "guestroom", "basement", "prefarea")])
prices_tree_rmse <- mean(sqrt(abs(dt_houses$price - prices_tree_pred)))

prices_tree_rmse
```


## Comparing two models

price linear model has a RMSE of 

```{r}
price_lm_rmse
```

price tree model has a RMSE of 

```{r}
prices_tree_rmse
```


It is surprising for me, as for a person who does not have a lot of experience in modelling, that linear model performs better than tree model by approx. 7.28%. 

```{r}
100 - price_lm_rmse / prices_tree_rmse * 100
```

collecting data for my statistics in the end

```{r}
dt_features_performance$price_lm_rmse[dt_features_performance$feature == 0] <- price_lm_rmse
dt_features_performance$price_tree_rmse[dt_features_performance$feature == 0] <- prices_tree_rmse
```


# Feature Engineering

Here I would like to try all ideas and observations, which I've had through my course work. As I have a lot of binary variables and they are already encoded by R's lm library I would use factor variables such as bedrooms, bathrooms and stories.

## Feature 1
#### Bedrooms as a factor variable

I want to use bedroom variable as a factor, to do that  I will delete original bedrooms and add instead bedrooms factor.

### Linear Model

Let's try Model with a new factor variable.

```{r}
price_lm <- lm(formula = price ~ area + factor(bedrooms) + hotwaterheating + airconditioning + stories + mainroad + parking + furnishingstatus + bathrooms + guestroom + basement + prefarea, data = dt_houses)

summary(price_lm)
```

```{r}
price_lm_rmse <- mean(sqrt(abs(price_lm$residuals)))

price_lm_rmse
```

Now the RMSE is a little bit better. Approximately by 0.12%. Next I am going to test this variable on the tree model.

```{r}
100 - price_lm_rmse / 797.382 * 100
```


### Tree Model

```{r}
prices_tree <- rpart(data = dt_houses, formula = price ~ area + factor(bedrooms) + hotwaterheating + airconditioning + stories + mainroad + parking + furnishingstatus + bathrooms + guestroom + basement + prefarea, method = 'anova')

prp(prices_tree, digits = -3)
```
I think, that in feature enginieering, I won't plot any tree complexity and explore tree itself, because here the main focus is on the benchmarking and comparing two models with new features. Let's prune the model and measure RMSE

```{r}
# pruning
prices_tree_min_cp <- prices_tree$cptable[which.min(prices_tree$cptable[, "xerror"]), "CP"]
model_tree <- prune(prices_tree, cp = prices_tree_min_cp )

# predicting
prices_tree_pred <- predict(prices_tree, dt_houses[, c("area","bathrooms", "bedrooms", "hotwaterheating", "airconditioning", "parking", "stories", "mainroad", "furnishingstatus", "guestroom", "basement", "prefarea")])

#calculating RMSE
prices_tree_rmse <- mean(sqrt(abs(dt_houses$price - prices_tree_pred)))

prices_tree_rmse
```

It performs the same, and I think it should be like that, because tree is not sensible for factor variables. It is still a number. It is interpreted other way by linear Model, but for the tree it is the same.

Here I am adding my variables to create a small chart in the end.
```{r}
dt_features_performance$price_lm_rmse[dt_features_performance$feature == 1] <- price_lm_rmse
dt_features_performance$price_tree_rmse[dt_features_performance$feature == 1] <- prices_tree_rmse
```

### Comparing

With new 'factor(bedroom)' feature, linear model performs better: 796.3947 - RMSE
On the other side tree model with new feature has not improved.

Linear model is still better, but may be there is some chances, we have 3 more features.


## Feature 2
#### Moving area closer to Gaussian (log transformation)

what if we will try to bring the area variable closer to Gaussian with log transformation, because area density is skewed to the left, log transformation can help us to normalize the variable.

### Linear Model

I am going to create a new column in my data table, which will be called area_log and will contain ln(area[i]), where i is the index of the row.
```{r}
dt_houses[, area_log := log(area)]
```


Let's also visualize it. Here function looks much balanced and I think it will work better.

```{r}
ggplot(data = dt_houses, aes(x = area_log)) + 
  geom_density(fill="#f1b147", color="#f1b147", alpha=0.25) + 
  labs(
    x = 'Price',
    y = 'Density'
  ) +
  theme_minimal() + 
  theme(axis.line = element_line(color = "#000000"))
```

Now I want to run the model with a second new feature.

```{r}
price_lm <- lm(formula = price ~ area + factor(bedrooms) + hotwaterheating + airconditioning + stories + mainroad + parking + furnishingstatus + bathrooms + guestroom + basement + prefarea + area_log, data = dt_houses)

summary(price_lm)
price_lm_rmse <- mean(sqrt(abs(price_lm$residuals)))
```

```{r}
price_lm_rmse
```


Success! It makes less errors. Previous we had RMSE of 796.3947, now it is 792.3163. Also 0.51% performance improvement.

```{r}
100 - price_lm_rmse / 796.3947 * 100
```


### Tree Model

It is tree's turn, I want to run new feature on tree Model.

```{r}
prices_tree <- rpart(data = dt_houses, formula = price ~ area + factor(bedrooms) + hotwaterheating + airconditioning + stories + mainroad + parking + furnishingstatus + bathrooms + guestroom + basement + prefarea  + area_log, method = 'anova')

prp(prices_tree, digits = -3)
```
Now I will prune the tree and then calculate the RMSE for the model with this new feature.

```{r}
prices_tree_min_cp <- prices_tree$cptable[which.min(prices_tree$cptable[, "xerror"]), "CP"]
model_tree <- prune(prices_tree, cp = prices_tree_min_cp )
prp(prices_tree,digits = -3)
```

and calculating the error

```{r}
prices_tree_pred <- predict(prices_tree, dt_houses[, c("area","bathrooms", "bedrooms", "hotwaterheating", "airconditioning", "parking", "stories", "mainroad", "furnishingstatus", "guestroom", "basement", "prefarea", "area_log")])
prices_tree_rmse <- mean(sqrt(abs(dt_houses$price - prices_tree_pred)))

prices_tree_rmse
```


Yep, there is no gain in performance, and I could probably say why. Linear model gains performance when we normalize variables, because this algorithm is sensitive to Gaussian, but the tree model, does not "care" so much about density of the variables, because it does not calculate "distance" between points and it is great in working with non-linar dependencies. This is my prediction, but I could be also wrong.

### Comparing

So there are RMSE from linear model: 792.3163 and RMSE from tree: 850.561. Linear model's performance is better than tree's

```{r}
dt_features_performance$price_lm_rmse[dt_features_performance$feature == 2] <- price_lm_rmse
dt_features_performance$price_tree_rmse[dt_features_performance$feature == 2] <- prices_tree_rmse
```

### Relevant statistics theory

I think, this could be a good Idea to take a loot at a correlation between variables, but from Data exploration I can already say, that area correlates with price.

Here we are, correlation plot:

```{r}
ggcorrplot(corr = cor(dt_houses[, .(price, area, bedrooms, bathrooms, stories, parking)]), 
           hc.order = TRUE,
           lab = TRUE)
```

Hm, correlation plot does not look as great, as I have expected, but the strongest correlation with price is area and amount of bathrooms. 

## Feature 3
##### Treat bathrooms as a factor variable

I got an Idea, we have bathrooms, and they are in range from 1 to 4.What if we will treat each amount of bathrooms as a factor variable. Because it is possible that home with 2 bathrooms is drastically more expensive than a house with 1, and the one with 3 bathrooms is super costly

### Linear Model


```{r}
price_lm <- lm(formula = price ~ area + factor(bedrooms) + hotwaterheating + airconditioning + stories + mainroad + parking + furnishingstatus + factor(bathrooms) + guestroom + basement + prefarea + area_log, data = dt_houses)

summary(price_lm)
price_lm_rmse <- mean(sqrt(abs(price_lm$residuals)))
```

```{r}
price_lm_rmse
```

And we gain a little bit more performance. This is really great. Approx 1.19% better than the first model without features. But not every factor is used, may be there is a big difference between 1 and 2 bathrooms, That may be because the luck of data, because I have less than 5 units with 3 or 4 bathrooms overall in my dataset.

```{r}
100 - price_lm_rmse / 797.382 * 100
```

### Tree Model

Let us try tree model now.

```{r}
prices_tree <- rpart(data = dt_houses, formula = price ~ area + factor(bedrooms) + hotwaterheating + airconditioning + stories + mainroad + parking + furnishingstatus + factor(bathrooms) + guestroom + basement + prefarea  + area_log, method = 'anova')

prp(prices_tree, digits = -3)
```

Now we have to prune the tree and then make predictions with RMSE calculations.

```{r}
prices_tree_min_cp <- prices_tree$cptable[which.min(prices_tree$cptable[, "xerror"]), "CP"]
model_tree <- prune(prices_tree, cp = prices_tree_min_cp )


prices_tree_pred <- predict(prices_tree, dt_houses[, c("area","bathrooms", "bedrooms", "hotwaterheating", "airconditioning", "parking", "stories", "mainroad", "furnishingstatus", "guestroom", "basement", "prefarea", "area_log")])
prices_tree_rmse <- mean(sqrt(abs(dt_houses$price - prices_tree_pred)))

prices_tree_rmse
```

This is awesome, we are making ~ 17.8951 less errors, this is 2.08% less errors.

```{r}
100 - prices_tree_rmse / 860.0223 * 100
```


### Comparing

This becomes interesting. While linear model has improved by 1.19%, tree model made bigger gain in performance: ~2.08%. This is ~2 times linear model gains.

```{r}
dt_features_performance$price_lm_rmse[dt_features_performance$feature == 3] <- price_lm_rmse
dt_features_performance$price_tree_rmse[dt_features_performance$feature == 3] <- prices_tree_rmse
```


## Feature 4
#### Stories as a factor variable

I want to treat my stories also as a categorical factor variable and to do that I will again use factor() function in R. I will try this feature in linear and tree model.

First, I would like to start as usual with linear model.

### Linear model

```{r}
# calculating and running model
price_lm <- lm(formula = price ~ area + factor(bedrooms) + hotwaterheating + airconditioning + factor(stories) + mainroad + parking + furnishingstatus + factor(bathrooms) + guestroom + basement + prefarea + area_log, data = dt_houses)

summary(price_lm)
```

```{r}
price_lm_rmse <- mean(sqrt(abs(price_lm$residuals)))
price_lm_rmse
```

There is a little performance gain in linear model. It is 0.37%. Now let's test this feature on Tree.

```{r}
100 - price_lm_rmse / 787.8693 * 100
```


### Tree Model

After my previous experience with bathrooms, I think here we could also see some performance gain, and I think it will be probably better than by linear model. I want now to run the tree model and then prune and predict, then measure the RMSE:

```{r}
prices_tree <- rpart(data = dt_houses, formula = price ~ area + factor(bedrooms) + hotwaterheating + airconditioning + factor(stories) + mainroad + parking + furnishingstatus + factor(bathrooms) + guestroom + basement + prefarea + area_log, method = 'anova')

prp(prices_tree, digits = -3)
```

Now I am pruning and predicting. Then calculating RMSE, which I want to compare with the previous result.

```{r}
# pruning 
prices_tree_min_cp <- prices_tree$cptable[which.min(prices_tree$cptable[, "xerror"]), "CP"]
model_tree <- prune(prices_tree, cp = prices_tree_min_cp )

# predicting
prices_tree_pred <- predict(prices_tree, dt_houses[, c("area","bathrooms", "bedrooms", "hotwaterheating", "airconditioning", "parking", "stories", "mainroad", "furnishingstatus", "guestroom", "basement", "prefarea", "area_log")])

# calculating RMSE
prices_tree_rmse <- mean(sqrt(abs(dt_houses$price - prices_tree_pred)))

prices_tree_rmse
```


and it seems that, there is no gain in performance if we use stories as a factor in the tree model.

### Comparing

With feature number 4, we have seen the gain in linear model, but the tree model, did not gain any performance. I think that the stories were not very strong variable in Tree model and in Linear model also, because in compare with such a variable like bathrooms, stories has gained 3 times for linear model and nothing for tree model.

After we included feature 4, linear model is still performing better than the tree model with the overall performance of 784.923 and 842.1271 respectively.

```{r}
dt_features_performance$price_lm_rmse[dt_features_performance$feature == 4] <- price_lm_rmse
dt_features_performance$price_tree_rmse[dt_features_performance$feature == 4] <- prices_tree_rmse
```

Now when I have my data, this is my conclusion plot:

```{r}
ggplot() + 
  geom_point(data = dt_features_performance, aes(x = feature, y = price_lm_rmse), 
             size = 4, color = "#1f77b4", alpha = 0.8) + 
  geom_line(data = dt_features_performance, aes(x = feature, y = price_lm_rmse), 
            color = "#1f77b4", linewidth = 1) + 
  geom_point(data = dt_features_performance, aes(x = feature, y = price_tree_rmse), 
             size = 4, color = "#ff7f0e", alpha = 0.8) + 
  geom_line(data = dt_features_performance, aes(x = feature, y = price_tree_rmse), 
            color = "#ff7f0e", linewidth = 1) + 
  labs(title = "Performance with Amount of Features", 
       x = "Amount of Features", 
       y = "Performance (RMSE)") + 
  theme_minimal() + 
  theme(
    axis.line = element_line(color = "#000000"),
    text = element_text(size = 14),
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5)
  )
```
With this plot I wanted to show you my progress in creating features. On the x-axis you can see the number of features, from 0 to 4, because I have included the raw model in this plot. On the y-axis is the RMSE of the models. 

The orange line is the tree model and the blue line is the linear model. 

With more features overall, both models could perform better, but for this dataset and my implementation, the linear model performs better. When I started, I thought the tree model would perform much better. 

Finally, I would like to mention that the tree model is lower in performance, but there was a really good boost with feature number 3. While all 4 features have improved the linear model incrementally with each new feature. 

***




